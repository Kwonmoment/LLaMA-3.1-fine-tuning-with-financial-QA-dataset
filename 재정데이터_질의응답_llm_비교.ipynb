{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwonmoment/LLaMA-3.1-fine-tuning-with-financial-QA-dataset/blob/main/%EC%9E%AC%EC%A0%95%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A7%88%EC%9D%98%EC%9D%91%EB%8B%B5_llm_%EB%B9%84%EA%B5%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "!apt-get -y install cmake\n",
        "!pip install llama-cpp-python langchain langchain-community huggingface_hub"
      ],
      "metadata": {
        "id": "OiKrERZLi7Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "# import llama_cpp\n",
        "from llama_cpp import Llama\n",
        "from huggingface_hub import login, hf_hub_download\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUlZA-bkqjdG",
        "outputId": "5a7a3bb6-bfcd-4e32-ac70-0bdc255fa846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanila  LLaMA 3.1 Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞"
      ],
      "metadata": {
        "id": "ZsZ8gzr9cZED"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB-okzBZiofP",
        "outputId": "5bd9dfdf-d1c6-4db5-fc7c-6f79c1ea3434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\n",
        "# 8-bit quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_has_fp16_weight=False,\n",
        ")\n",
        "\n",
        "vanila_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    quantization_config = quantization_config,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ïû¨Ï†ï Îç∞Ïù¥ÌÑ∞Î•º ÌôúÏö©Ìï¥ Fine-tuning Îêú Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞"
      ],
      "metadata": {
        "id": "_GqxsXMuYvT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "login(\"\")\n",
        "\n",
        "# download fine_tuned model\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"SoonKwan/model\",\n",
        "    filename=\"unsloth.Q8_0.gguf\"\n",
        ")\n",
        "\n",
        "fine_tuned_model = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=2048,  # context window\n",
        "    n_gpu_layers=-1  # auto detect GPU layers\n",
        ")"
      ],
      "metadata": {
        "id": "kZNWeqBuqvMS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(instruction, input_text=\"\", output_text=\"\"):\n",
        "\n",
        "    prompt = alpaca_prompt.format(instruction, input_text, output_text)\n",
        "\n",
        "\n",
        "    output = fine_tuned_model(\n",
        "        prompt,\n",
        "        max_tokens=128,\n",
        "        temperature=0.7,\n",
        "        echo=True\n",
        "    )\n",
        "\n",
        "    return output['choices'][0]['text'].strip()"
      ],
      "metadata": {
        "id": "iSNpyCd-tfLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alpaca ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø Ï†ïÏùò"
      ],
      "metadata": {
        "id": "p8o1vk5jgay3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "F3q52HTcgXYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ïû¨Ï†ï Îç∞Ïù¥ÌÑ∞ ÏßàÏùòÏùëÎãµ Îç∞Ïù¥ÌÑ∞ÏÖã Î∂àÎü¨Ïò§Í∏∞"
      ],
      "metadata": {
        "id": "3PWdqhaSmhkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"csv\", data_files=\"train.csv\", split = \"train\")"
      ],
      "metadata": {
        "id": "QetjHKHpjiTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = dataset['Question'][0]"
      ],
      "metadata": {
        "id": "5ZNeogW3gkEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Response"
      ],
      "metadata": {
        "id": "7AFul_TBkKNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['Answer'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OOgUjfUbkIkv",
        "outputId": "3d0a4d52-49d8-45e3-da38-b584287985fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2024ÎÖÑ Ï§ëÏïôÏ†ïÎ∂Ä Ïû¨Ï†ïÏ≤¥Í≥ÑÎäî ÏòàÏÇ∞(ÏùºÎ∞ò¬∑ÌäπÎ≥ÑÌöåÍ≥Ñ)Í≥º Í∏∞Í∏àÏúºÎ°ú Íµ¨Î∂ÑÎêòÎ©∞, 2024ÎÖÑ Í∏∞Ï§ÄÏúºÎ°ú ÏùºÎ∞òÌöåÍ≥Ñ 1Í∞ú, ÌäπÎ≥ÑÌöåÍ≥Ñ 21Í∞ú, Í∏∞Í∏à 68Í∞úÎ°ú Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanila LLaMA 3.1 Response (1)"
      ],
      "metadata": {
        "id": "WZRAZHppkNAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "FastLanguageModel.for_inference(vanila_model)\n",
        "vanila_inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction, # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "vanila_outputs = vanila_model.generate(\n",
        "    **vanila_inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 128,\n",
        "      )\n",
        "\n",
        "vanila_response = tokenizer.decode(vanila_outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "EX6lsjEFgshO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vanila_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp2vzu2Sgscs",
        "outputId": "87666434-3241-402f-a0d5-c0a2e6ad0468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "2024ÎÖÑ Ï§ëÏïôÏ†ïÎ∂Ä Ïû¨Ï†ïÏ≤¥Í≥ÑÎäî Ïñ¥ÎñªÍ≤å Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÎÇòÏöî?\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning LLaMA 3.1 Response (1)"
      ],
      "metadata": {
        "id": "sFC8MP_QkSd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_response = generate_response(instruction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyYQ6LUbbpVc",
        "outputId": "c3937c73-7a92-405f-bbc1-610b8b1c0c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   34195.06 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    53 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   97305.10 ms /   102 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fine_tuned_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y_pMO4Fgn9D",
        "outputId": "2dc38a1e-1e1b-41a4-8c0b-9899556be3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "2024ÎÖÑ Ï§ëÏïôÏ†ïÎ∂Ä Ïû¨Ï†ïÏ≤¥Í≥ÑÎäî Ïñ¥ÎñªÍ≤å Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÎÇòÏöî?\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "Ïû¨Ï†ïÏ≤¥Í≥ÑÎäî ÏùºÎ∞òÌöåÍ≥Ñ, ÌäπÎ≥ÑÌöåÍ≥Ñ, ÏÇ¨Î¶ΩÌïôÍµêÌäπÎ≥ÑÌöåÍ≥Ñ, Î∂ÄÏ≤òÌäπÎ≥ÑÌöåÍ≥Ñ, ÏÇ¨Î¶ΩÎåÄÌïôÌäπÎ≥ÑÌöåÍ≥Ñ, Íµ≠Î¶ΩÎåÄÌïôÌäπÎ≥ÑÌöåÍ≥Ñ, Í∏∞ÌÉÄÌäπÎ≥ÑÌöåÍ≥ÑÎ°ú Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insturction Î≥ÄÌòï"
      ],
      "metadata": {
        "id": "KyXoBPTRn0uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"2024ÎÖÑ Ï§ëÏïôÏ†ïÎ∂Ä Ïû¨Ï†ïÏ≤¥Í≥Ñ Íµ¨ÏÑ±?\""
      ],
      "metadata": {
        "id": "aewb8wtgnxgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanila LLaMA 3.1 Response (2)"
      ],
      "metadata": {
        "id": "MIgL6Bw6oWLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "FastLanguageModel.for_inference(vanila_model)\n",
        "vanila_inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction, # instruction\n",
        "        \"\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "vanila_outputs = vanila_model.generate(\n",
        "    **vanila_inputs,\n",
        "    streamer = text_streamer,\n",
        "    max_new_tokens = 128,\n",
        "      )\n",
        "\n",
        "vanila_response = tokenizer.decode(vanila_outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "0zVzedK4oCCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vanila_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlWpDzD-oGqr",
        "outputId": "4424699c-4892-4781-ed52-05bdfea19806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "2024ÎÖÑ Ï§ëÏïôÏ†ïÎ∂Ä Ïû¨Ï†ïÏ≤¥Í≥Ñ Íµ¨ÏÑ±?\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "```\n",
            "2024ÎÖÑ Ï§ëÏïôÏ†ïÎ∂Ä Ïû¨Ï†ïÏ≤¥Í≥Ñ Íµ¨ÏÑ±ÏùÄ Îã§ÏùåÍ≥º Í∞ôÎã§.\n",
            "1. Ï§ëÏïôÏ†ïÎ∂Ä ÏòàÏÇ∞\n",
            "2. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "3. ÏßÄÎ∞©Ï†ïÎ∂Ä ÏòàÏÇ∞\n",
            "4. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "5. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "6. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "7. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "8. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "9. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "10. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "11. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "12. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "13. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "14. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "15. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "16. Íµ≠Ìöå ÏòàÏÇ∞\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning LLaMA 3.1 Response (2)"
      ],
      "metadata": {
        "id": "XD1GfptqoQxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_response = generate_response(instruction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWlwvHazoJUL",
        "outputId": "3e093be1-dc92-4d17-a344-dcce3af3b842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 40 prefix-match hit, remaining 8 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   34195.06 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   63882.01 ms /    61 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fine_tuned_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiMP4hGLoM0H",
        "outputId": "29bafcf0-d4da-40b8-c154-c73f44b10df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "2024ÎÖÑ Ï§ëÏïôÏ†ïÎ∂Ä Ïû¨Ï†ïÏ≤¥Í≥Ñ Íµ¨ÏÑ±?\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "Ï§ëÏïôÏ†ïÎ∂Ä Ïû¨Ï†ïÏ≤¥Í≥ÑÎäî ÏùºÎ∞òÌöåÍ≥Ñ, ÏùºÎ∞òÌöåÍ≥Ñ ÌäπÎ≥ÑÌöåÍ≥Ñ(ÏÑ∏Í¥ÄÍ≥†Í≥ºÍ∏à, ÏÑ∏ÏûÖÎ≥¥Ï†ÑÍ∏à, Íµ≠Í∞ÄÏû¨Ï†ïÏö¥Ïö©ÎπÑ)ÏôÄ ÌäπÎ≥ÑÌöåÍ≥Ñ(ÏùºÎ∞ò, ÏßÄÎ∞©)Î°ú Íµ¨Î∂ÑÎê©ÎãàÎã§.\n"
          ]
        }
      ]
    }
  ]
}